"""
Coverage Extraction Pipeline

Purpose: Extract coverage entities from document_clause table
Strategy:
  1. Extract coverage names from proposal documents (structured_data)
  2. Extract coverage constraints from business_spec tables
  3. Populate coverage table with metadata

Design: Phase 2.1 of TODO.md
"""

import os
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import List, Dict, Optional
import logging
import argparse
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CoveragePipeline:
    """Extract coverage entities from ingested documents"""

    def __init__(self, db_url: str):
        self.db_url = db_url

    def extract_coverages_from_proposals(self, carrier: Optional[str] = None) -> List[Dict]:
        """
        Extract unique coverage names from proposal, product_summary, and business_spec documents

        Per carrier_analysis_report.md:
        - Proposals have 26.3% of coverage data
        - Product summaries have 15.8%
        - Business specs have 5.3%
        - Easy summaries have 2.6%

        Args:
            carrier: Filter by carrier (e.g., 'samsung', 'db', 'lotte')

        Returns:
            List of coverage dictionaries with metadata
        """
        conn = psycopg2.connect(self.db_url)
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # Build query with optional carrier filter
        # Extract from ALL document types that contain coverage data
        query = """
            SELECT DISTINCT
                c.company_code,
                p.product_code,
                p.id as product_id,
                d.doc_type,
                dc.structured_data->>'coverage_name' as coverage_name,
                dc.structured_data->>'coverage_amount_text' as coverage_amount_text,
                dc.structured_data->>'premium' as premium,
                dc.structured_data->>'premium_frequency' as premium_frequency
            FROM document_clause dc
            JOIN document d ON dc.document_id = d.id
            JOIN product p ON d.product_id = p.id
            JOIN company c ON p.company_id = c.id
            WHERE d.doc_type IN ('proposal', 'product_summary', 'business_spec', 'easy_summary')
              AND dc.clause_type = 'table_row'
              AND dc.structured_data IS NOT NULL
              AND dc.structured_data->>'coverage_name' IS NOT NULL
        """

        params = []
        if carrier:
            query += " AND c.company_code = %s"
            params.append(carrier)

        query += " ORDER BY c.company_code, p.product_code, coverage_name"

        cur.execute(query, params)
        results = cur.fetchall()

        cur.close()
        conn.close()

        # Log extraction stats by doc_type
        doc_type_counts = {}
        for row in results:
            doc_type = row['doc_type']
            doc_type_counts[doc_type] = doc_type_counts.get(doc_type, 0) + 1

        logger.info(f"Extracted {len(results)} coverage entries from all document types:")
        for doc_type, count in sorted(doc_type_counts.items()):
            logger.info(f"  {doc_type}: {count} coverages")

        return [dict(row) for row in results]

    def save_coverages(self, coverages: List[Dict]) -> int:
        """
        Save unique coverages to coverage table

        Args:
            coverages: List of coverage dictionaries

        Returns:
            Number of coverages inserted
        """
        conn = psycopg2.connect(self.db_url)
        cur = conn.cursor()

        inserted = 0
        skipped_invalid = 0

        for cov in coverages:
            try:
                # Clean coverage name and extract metadata (problem.md fix)
                cleaned = self._clean_coverage_name(cov['coverage_name'])

                # Skip invalid coverage names (period-only, too short, etc.)
                if not cleaned['is_valid']:
                    skipped_invalid += 1
                    logger.debug(f"Skipped invalid coverage: {cov['coverage_name']}")
                    continue

                # Use cleaned coverage_name
                coverage_name = cleaned['coverage_name']
                clause_number = cleaned['clause_number']
                coverage_period = cleaned['coverage_period']

                # Determine renewal_type (cleaned value takes precedence)
                renewal_type = cleaned['renewal_type'] or self._extract_renewal_type(coverage_name)

                # Insert coverage with new columns
                cur.execute("""
                    INSERT INTO coverage (
                        product_id,
                        coverage_code,
                        coverage_name,
                        coverage_category,
                        renewal_type,
                        is_basic,
                        clause_number,
                        coverage_period
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (product_id, coverage_code) DO NOTHING
                    RETURNING id
                """, (
                    cov['product_id'],
                    self._generate_coverage_code(coverage_name),
                    coverage_name,
                    self._infer_coverage_category(coverage_name),
                    renewal_type,
                    'ê¸°ë³¸ê³„ì•½' in coverage_name,
                    clause_number,
                    coverage_period
                ))

                result = cur.fetchone()
                if result:
                    inserted += 1
                    logger.debug(f"Inserted coverage: {coverage_name} (id={result[0]})")

            except Exception as e:
                logger.error(f"Failed to insert coverage {cov['coverage_name']}: {e}")
                continue

        conn.commit()
        cur.close()
        conn.close()

        logger.info(f"Inserted {inserted} new coverages (skipped {skipped_invalid} invalid)")
        return inserted

    def _generate_coverage_code(self, coverage_name: str) -> str:
        """
        Generate coverage_code from coverage_name

        Examples:
            "ì¼ë°˜ì•”ì§„ë‹¨ë¹„â…¡" â†’ "ì¼ë°˜ì•”ì§„ë‹¨ë¹„2"
            "ìƒí•´ì‚¬ë§" â†’ "ìƒí•´ì‚¬ë§"
            "ê°‘ìƒì„ ì•”Â·ê¸°íƒ€í”¼ë¶€ì•”Â·ìœ ì‚¬ì•”ì§„ë‹¨ë¹„" â†’ "ê°‘ìƒì„ ì•”ê¸°íƒ€í”¼ë¶€ì•”ìœ ì‚¬ì•”ì§„ë‹¨ë¹„"
        """
        import unicodedata
        import re

        # Normalize to NFC
        normalized = unicodedata.normalize('NFC', coverage_name)

        # Remove special characters but keep Korean, English, numbers
        # Replace Â· with empty string
        cleaned = normalized.replace('Â·', '').replace('(', '').replace(')', '')

        # Convert Roman numerals to Arabic
        cleaned = cleaned.replace('â… ', '1').replace('â…¡', '2').replace('â…¢', '3').replace('â…£', '4')

        # Remove any remaining special characters except Korean/English/numbers
        cleaned = re.sub(r'[^\wê°€-íž£]', '', cleaned)

        return cleaned

    def _infer_coverage_category(self, coverage_name: str) -> str:
        """
        Infer coverage category from coverage name

        Returns:
            Category string matching coverage_category table
        """
        # Cancer-related
        if 'ì•”' in coverage_name and 'ì§„ë‹¨' in coverage_name:
            return 'cancer_diagnosis'
        # Major diseases (stroke, heart)
        elif ('ë‡Œ' in coverage_name or 'ì‹¬ê·¼ê²½ìƒ‰' in coverage_name or 'í—ˆí˜ˆ' in coverage_name) and 'ì§„ë‹¨' in coverage_name:
            return 'major_disease_diagnosis'
        # Death/Disability
        elif 'ì‚¬ë§' in coverage_name or 'ìž¥í•´' in coverage_name or 'ìž¥ì• ' in coverage_name:
            return 'death_disability'
        # Hospitalization
        elif 'ìž…ì›' in coverage_name:
            return 'hospitalization'
        # Surgery
        elif 'ìˆ˜ìˆ ' in coverage_name:
            return 'surgery'
        # Outpatient
        elif 'í†µì›' in coverage_name:
            return 'outpatient'
        # Specific disease diagnosis
        elif 'ì§„ë‹¨' in coverage_name:
            return 'specific_disease_diagnosis'
        else:
            return 'other_benefits'

    def _extract_renewal_type(self, coverage_name: str) -> Optional[str]:
        """
        Extract renewal type from coverage name

        Returns:
            'ê°±ì‹ í˜•', 'ë¹„ê°±ì‹ í˜•', or None
        """
        if 'ê°±ì‹ í˜•' in coverage_name:
            return 'ê°±ì‹ í˜•'
        elif 'ë¹„ê°±ì‹ í˜•' in coverage_name:
            return 'ë¹„ê°±ì‹ í˜•'
        return None

    def _clean_coverage_name(self, raw_name: str) -> Dict:
        """
        Clean coverage name and extract clause_number, coverage_period, renewal_type

        Handles data pollution patterns from problem.md:
        - "119 ë‡Œì¡¸ì¤‘ì§„ë‹¨ë¹„" â†’ clause_number: "119", coverage_name: "ë‡Œì¡¸ì¤‘ì§„ë‹¨ë¹„"
        - "10ë…„", "15ë…„" â†’ skip (period-only data)
        - "[ê°±ì‹ í˜•]ë‹´ë³´ëª…" â†’ renewal_type: "ê°±ì‹ í˜•", coverage_name: "ë‹´ë³´ëª…"

        Args:
            raw_name: Original coverage name from structured_data

        Returns:
            Dict with keys: coverage_name, clause_number, coverage_period, renewal_type, is_valid
        """
        import re

        result = {
            'coverage_name': raw_name,
            'clause_number': None,
            'coverage_period': None,
            'renewal_type': None,
            'is_valid': True
        }

        if not raw_name or not raw_name.strip():
            result['is_valid'] = False
            return result

        name = raw_name.strip()

        # Pattern 1: Period-only data ("10ë…„", "15ë…„", "20ë…„")
        if re.match(r'^(5|10|15|20|30)ë…„$', name):
            result['is_valid'] = False
            result['coverage_period'] = name
            return result

        # Pattern 2: Clause number prefix ("119 ë‡Œì¡¸ì¤‘ì§„ë‹¨ë¹„", "121 ë‡Œì¶œí˜ˆì§„ë‹¨ë¹„")
        clause_match = re.match(r'^(\d+)\s+(.+)$', name)
        if clause_match:
            result['clause_number'] = clause_match.group(1)
            name = clause_match.group(2).strip()

        # Pattern 3: Renewal type prefix ("[ê°±ì‹ í˜•]ë‹´ë³´ëª…", "[ë¹„ê°±ì‹ í˜•]ë‹´ë³´ëª…")
        if name.startswith('[ê°±ì‹ í˜•]'):
            result['renewal_type'] = 'ê°±ì‹ í˜•'
            name = name.replace('[ê°±ì‹ í˜•]', '').strip()
        elif name.startswith('[ë¹„ê°±ì‹ í˜•]'):
            result['renewal_type'] = 'ë¹„ê°±ì‹ í˜•'
            name = name.replace('[ë¹„ê°±ì‹ í˜•]', '').strip()

        # Pattern 4: Period prefix ("10ë…„í˜• ì•”ì§„ë‹¨ë¹„")
        period_match = re.match(r'^(\d+ë…„í˜•?)\s+(.+)$', name)
        if period_match:
            result['coverage_period'] = period_match.group(1)
            name = period_match.group(2).strip()

        # Validation: Too short name (< 3 characters)
        if len(name) < 3:
            result['is_valid'] = False

        # Validation: Pure number
        if name.isdigit():
            result['is_valid'] = False

        result['coverage_name'] = name
        return result

    def run(self, carrier: Optional[str] = None) -> Dict:
        """
        Run coverage extraction pipeline

        Args:
            carrier: Optional carrier filter

        Returns:
            Summary dictionary
        """
        logger.info(f"Starting coverage extraction pipeline{' for ' + carrier if carrier else ''}...")

        # 1. Extract coverages from proposals
        coverages = self.extract_coverages_from_proposals(carrier)

        # 2. Save to coverage table
        inserted = self.save_coverages(coverages)

        summary = {
            'total_extracted': len(coverages),
            'inserted': inserted,
            'carrier': carrier or 'all'
        }

        logger.info(f"âœ… Coverage extraction complete: {inserted}/{len(coverages)} inserted")
        return summary


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Extract coverages from proposal documents')
    parser.add_argument('--carrier', help='Filter by carrier (e.g., samsung, db, lotte)')
    parser.add_argument('--mode', default='extract_toc',
                        help='Mode: extract_toc (default) or extract_constraints')

    args = parser.parse_args()

    db_url = os.getenv('POSTGRES_URL')
    if not db_url:
        raise ValueError("POSTGRES_URL environment variable is required. Check .env file.")

    pipeline = CoveragePipeline(db_url)

    # Handle "all" carrier as None (no filter)
    carrier_filter = None if args.carrier == 'all' else args.carrier

    if args.mode == 'extract_toc':
        summary = pipeline.run(carrier=carrier_filter)
        print(f"\nðŸ“Š Summary:")
        print(f"  Carrier: {summary['carrier']}")
        print(f"  Total extracted: {summary['total_extracted']}")
        print(f"  Inserted: {summary['inserted']}")
    else:
        logger.warning(f"Mode '{args.mode}' not yet implemented")


if __name__ == '__main__':
    main()
