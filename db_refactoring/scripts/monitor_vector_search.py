#!/usr/bin/env python3
"""
ë²¡í„° ê²€ìƒ‰ ëª¨ë‹ˆí„°ë§ ë° ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

ìš©ë„: pgvector HNSW ì¸ë±ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë²¤ì¹˜ë§ˆí¬
ì‹¤í–‰: python db_refactoring/scripts/monitor_vector_search.py [--benchmark] [--stats]
"""

import os
import sys
import time
import json
import argparse
import statistics
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

import psycopg2
from psycopg2.extras import RealDictCursor


class VectorSearchMonitor:
    """ë²¡í„° ê²€ìƒ‰ ëª¨ë‹ˆí„°ë§ ë° ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self):
        self.postgres_url = os.getenv('POSTGRES_URL')
        if not self.postgres_url:
            raise ValueError("POSTGRES_URL í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def get_connection(self):
        """DB ì—°ê²° ìƒì„±"""
        return psycopg2.connect(self.postgres_url)

    def get_index_stats(self) -> dict:
        """ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ"""
        conn = self.get_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)

        stats = {}

        # í…Œì´ë¸” í†µê³„
        cur.execute("""
            SELECT
                COUNT(*) as total_embeddings,
                COUNT(embedding) as with_embedding,
                COUNT(DISTINCT model_name) as model_count
            FROM clause_embedding
        """)
        stats['table'] = dict(cur.fetchone())

        # ëª¨ë¸ë³„ í†µê³„
        cur.execute("""
            SELECT model_name, COUNT(*) as count
            FROM clause_embedding
            WHERE model_name IS NOT NULL
            GROUP BY model_name
            ORDER BY count DESC
        """)
        stats['models'] = [dict(row) for row in cur.fetchall()]

        # ì¸ë±ìŠ¤ í†µê³„
        cur.execute("""
            SELECT
                i.indexname as name,
                pg_size_pretty(pg_relation_size(c.oid)) as size,
                pg_relation_size(c.oid) as size_bytes,
                am.amname as type
            FROM pg_indexes i
            JOIN pg_class c ON c.relname = i.indexname
            JOIN pg_am am ON am.oid = c.relam
            WHERE i.tablename = 'clause_embedding'
            ORDER BY pg_relation_size(c.oid) DESC
        """)
        stats['indexes'] = [dict(row) for row in cur.fetchall()]

        # í…Œì´ë¸” í¬ê¸°
        cur.execute("""
            SELECT
                pg_size_pretty(pg_total_relation_size('clause_embedding')) as total_size,
                pg_size_pretty(pg_table_size('clause_embedding')) as table_size,
                pg_size_pretty(pg_indexes_size('clause_embedding')) as index_size
        """)
        stats['sizes'] = dict(cur.fetchone())

        cur.close()
        conn.close()

        return stats

    def get_sample_embeddings(self, limit: int = 5) -> list:
        """ìƒ˜í”Œ ì„ë² ë”© ì¡°íšŒ (ë²¤ì¹˜ë§ˆí¬ìš©)"""
        conn = self.get_connection()
        cur = conn.cursor()

        cur.execute("""
            SELECT id, embedding
            FROM clause_embedding
            WHERE embedding IS NOT NULL
            ORDER BY RANDOM()
            LIMIT %s
        """, (limit,))

        embeddings = cur.fetchall()
        cur.close()
        conn.close()

        return embeddings

    def benchmark_search(self, num_queries: int = 10, top_k: int = 10,
                         ef_search_values: list = None) -> dict:
        """ë²¡í„° ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if ef_search_values is None:
            ef_search_values = [40, 100, 200]

        conn = self.get_connection()
        cur = conn.cursor()

        # ìƒ˜í”Œ ì¿¼ë¦¬ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        sample_embeddings = self.get_sample_embeddings(num_queries)

        if not sample_embeddings:
            return {'error': 'ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. build_indexë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.'}

        results = {}

        for ef_search in ef_search_values:
            cur.execute(f"SET hnsw.ef_search = {ef_search}")

            latencies = []

            for emb_id, embedding in sample_embeddings:
                # ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                embedding_str = str(list(embedding))

                start_time = time.perf_counter()

                cur.execute(f"""
                    SELECT id, embedding <=> %s::vector as distance
                    FROM clause_embedding
                    WHERE embedding IS NOT NULL
                    ORDER BY embedding <=> %s::vector
                    LIMIT %s
                """, (embedding_str, embedding_str, top_k))

                _ = cur.fetchall()
                end_time = time.perf_counter()

                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)

            results[ef_search] = {
                'ef_search': ef_search,
                'num_queries': len(latencies),
                'top_k': top_k,
                'latency_ms': {
                    'min': round(min(latencies), 2),
                    'max': round(max(latencies), 2),
                    'mean': round(statistics.mean(latencies), 2),
                    'median': round(statistics.median(latencies), 2),
                    'p95': round(sorted(latencies)[int(len(latencies) * 0.95)], 2) if len(latencies) >= 20 else None,
                }
            }

        cur.close()
        conn.close()

        return results

    def check_index_usage(self) -> dict:
        """ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€ í™•ì¸"""
        conn = self.get_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # ìƒ˜í”Œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        sample = self.get_sample_embeddings(1)

        if not sample:
            return {'error': 'ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}

        embedding_str = str(list(sample[0][1]))

        # EXPLAIN ì‹¤í–‰
        cur.execute(f"""
            EXPLAIN (FORMAT JSON)
            SELECT id, embedding <=> %s::vector as distance
            FROM clause_embedding
            ORDER BY embedding <=> %s::vector
            LIMIT 10
        """, (embedding_str, embedding_str))

        plan = cur.fetchone()

        cur.close()
        conn.close()

        # Index Scan í™•ì¸
        plan_json = plan[0] if isinstance(plan, tuple) else plan
        plan_str = json.dumps(plan_json)

        return {
            'uses_hnsw_index': 'idx_clause_embedding_hnsw' in plan_str or 'Index Scan' in plan_str,
            'plan': plan_json
        }

    def print_stats(self):
        """í†µê³„ ì¶œë ¥"""
        stats = self.get_index_stats()

        print("\n" + "=" * 60)
        print("ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ í†µê³„")
        print("=" * 60)

        print("\nğŸ“Š í…Œì´ë¸” í†µê³„:")
        print("-" * 40)
        print(f"  ì´ ì„ë² ë”© ìˆ˜: {stats['table']['total_embeddings']:,}")
        print(f"  ì„ë² ë”© ìˆìŒ: {stats['table']['with_embedding']:,}")
        print(f"  ëª¨ë¸ ìˆ˜: {stats['table']['model_count']}")

        if stats['models']:
            print("\nğŸ“Š ëª¨ë¸ë³„ í†µê³„:")
            print("-" * 40)
            for model in stats['models']:
                print(f"  {model['model_name'] or 'NULL'}: {model['count']:,}")

        print("\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
        print("-" * 40)
        for idx in stats['indexes']:
            print(f"  {idx['name']}")
            print(f"    íƒ€ì…: {idx['type']}, í¬ê¸°: {idx['size']}")

        print("\nğŸ“Š ì €ì¥ì†Œ í¬ê¸°:")
        print("-" * 40)
        print(f"  í…Œì´ë¸”: {stats['sizes']['table_size']}")
        print(f"  ì¸ë±ìŠ¤: {stats['sizes']['index_size']}")
        print(f"  ì „ì²´: {stats['sizes']['total_size']}")

        print("\n" + "=" * 60)

    def print_benchmark(self, num_queries: int = 10):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ë²¡í„° ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)

        # ì¸ë±ìŠ¤ ì‚¬ìš© í™•ì¸
        index_check = self.check_index_usage()
        if 'error' in index_check:
            print(f"\nâŒ {index_check['error']}")
            return

        print(f"\nğŸ” HNSW ì¸ë±ìŠ¤ ì‚¬ìš©: {'âœ… ì˜ˆ' if index_check['uses_hnsw_index'] else 'âŒ ì•„ë‹ˆì˜¤'}")

        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ({num_queries}ê°œ ì¿¼ë¦¬)...")
        results = self.benchmark_search(num_queries=num_queries)

        if 'error' in results:
            print(f"\nâŒ {results['error']}")
            return

        print("\n" + "-" * 60)
        print(f"{'ef_search':>10} {'min(ms)':>10} {'mean(ms)':>10} {'median(ms)':>12} {'max(ms)':>10}")
        print("-" * 60)

        for ef_search, data in sorted(results.items()):
            lat = data['latency_ms']
            print(f"{ef_search:>10} {lat['min']:>10.2f} {lat['mean']:>10.2f} {lat['median']:>12.2f} {lat['max']:>10.2f}")

        print("-" * 60)
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        print("  - ef_search=40: ê¸°ë³¸ê°’, ì†ë„ ìš°ì„ ")
        print("  - ef_search=100: ê· í˜•")
        print("  - ef_search=200: ì •í™•ë„ ìš°ì„ ")

        print("\n" + "=" * 60)

    def save_benchmark_result(self, num_queries: int = 20):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        stats = self.get_index_stats()
        benchmark = self.benchmark_search(num_queries=num_queries)

        result = {
            'timestamp': datetime.now().isoformat(),
            'stats': stats,
            'benchmark': benchmark,
            'config': {
                'num_queries': num_queries,
                'postgres_url': self.postgres_url.split('@')[1] if '@' in self.postgres_url else 'hidden'
            }
        }

        # ê²°ê³¼ ì €ì¥
        output_dir = project_root / 'db_refactoring' / 'test_results'
        output_dir.mkdir(exist_ok=True)

        output_file = output_dir / f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nâœ… ê²°ê³¼ ì €ì¥ë¨: {output_file}")

        return result


def main():
    parser = argparse.ArgumentParser(description='ë²¡í„° ê²€ìƒ‰ ëª¨ë‹ˆí„°ë§ ë° ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--stats', action='store_true', help='ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥')
    parser.add_argument('--benchmark', action='store_true', help='ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰')
    parser.add_argument('--save', action='store_true', help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥')
    parser.add_argument('--queries', type=int, default=10, help='ë²¤ì¹˜ë§ˆí¬ ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸: 10)')
    parser.add_argument('--json', action='store_true', help='JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥')

    args = parser.parse_args()

    monitor = VectorSearchMonitor()

    if args.json:
        result = {
            'stats': monitor.get_index_stats(),
        }
        if args.benchmark:
            result['benchmark'] = monitor.benchmark_search(num_queries=args.queries)
        print(json.dumps(result, indent=2, default=str))
    elif args.save:
        monitor.save_benchmark_result(num_queries=args.queries)
    elif args.benchmark:
        monitor.print_stats()
        monitor.print_benchmark(num_queries=args.queries)
    elif args.stats:
        monitor.print_stats()
    else:
        # ê¸°ë³¸: í†µê³„ë§Œ ì¶œë ¥
        monitor.print_stats()


if __name__ == '__main__':
    main()
